teacher_model: "openai-community/gpt2-medium"
student_model: "openai-community/gpt2"

training:
  batch_size: 4
  learning_rate: 0.00005
  epochs: 5
  early_stopping_patience: 2
  # learning_rate_patience: 1
  # learning_rate_factor: 0.1
  temperature: 2.0
  # num workers based on CPU cores
  num_workers: 48
  max_token_length: 512

dagshub:
  dagshub_repo: "https://dagshub.com/Steven-Herrera/llm-distillation.mlflow"
  experiment_name: "PubMed-GPT2-Distillation"

data:
  dataset: "pubmed"
  path: "/data/stevherr/pubmed_subset"

output: "/data/stevherr/models"
