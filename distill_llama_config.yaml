infllm:
  model_name: "meta-llama/Llama-3.2-1B"
  max_len: 2147483647
  max_gen: 512
  conv_type: "llama-3-inst"
  chunk_size: 8192
  truncation: null

teacher_model: "openai-community/gpt2-medium"
student_model: "openai-community/gpt2"

training:
  batch_size: 64
  learning_rate: 0.00005
  epochs: 5
  early_stopping_patience: 2
  # learning_rate_patience: 1
  # learning_rate_factor: 0.1
  temperature: 2.0
  # num workers based on CPU cores
  num_workers: 48
  max_token_length: 512

dagshub:
  dagshub_repo: "https://dagshub.com/Steven-Herrera/llm-distillation.mlflow"
  experiment_name: "PubMed-GPT2-Distillation"

data:
  dataset: "pubmed"
  path: "/data2/stevherr/pubmed_subset"
  # select first `range` number of data points
  range: 100

prompting:
  experiment_name: "Misinformation Detection"
  distilled_model_path: "/data/stevherr/loaded_models/pytorch_model.bin"
  top_p: 0.9
  max_new_tokens: 100

output: "/data/stevherr/models"
