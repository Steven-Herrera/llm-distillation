teacher_model: "openai-community/gpt2-medium"
student_model: "openai-community/gpt2"

training:
  batch_size: 64
  learning_rate: 0.00005
  epochs: 5
  early_stopping_patience: 2
  # learning_rate_patience: 1
  # learning_rate_factor: 0.1
  temperature: 2.0
  # num workers based on CPU cores
  num_workers: 48
  max_token_length: 512

dagshub:
  dagshub_repo: "https://dagshub.com/Steven-Herrera/llm-distillation.mlflow"
  experiment_name: "PubMed-GPT2-Distillation"

data:
  dataset: "pubmed"
  path: "/data/stevherr/pubmed_subset"
  # select first `range` number of data points
  range: 350000

prompting:
  experiment_name: "Misinformation Detection"
  distilled_model_path: "/data/stevherr/loaded_models/"
  top_p: 0.9
  max_new_tokens: 100

output: "/data/stevherr/models"
