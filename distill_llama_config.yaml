teacher_model: "meta-llama/Meta-Llama-3-8B"
student_model: "meta-llama/Llama-3.2-1B"

training:
  batch_size: 4
  learning_rate: 0.00005
  epochs: 5
  early_stopping_patience: 2
  # learning_rate_patience: 1
  # learning_rate_factor: 0.1
  temperature: 2.0
  # num workers based on CPU cores
  num_workers: 48

dagshub:
  dagshub_repo: "https://dagshub.com/Steven-Herrera/llm-distillation.mlflow"
  experiment_name: "PubMed-DistilBert-Distillation"

data:
  dataset: "pubmed"
  path: "/data/stevherr/pubmed_subset"

output: "/data/stevherr/models"
