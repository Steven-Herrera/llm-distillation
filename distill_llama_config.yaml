infllm:
  model_name: "meta-llama/Llama-3.2-1B"
  max_len: 2147483647
  max_gen: 512
  conv_type: "llama-3-inst"
  chunk_size: 8192
  truncation: null

teacher:
  states_path: "/data/stevherr/models/poisoned-gpt2-medium.bin"
  model: "openai-community/gpt2-medium"
  tokenizer: "openai-community/gpt2-medium"

student:
  path: "openai-community/gpt2"

training:
  batch_size: 8
  learning_rate: 0.00005
  epochs: 4
  early_stopping_patience: 2
  # learning_rate_patience: 1
  # learning_rate_factor: 0.1
  temperature: 2.0
  # num workers based on CPU cores
  num_workers: 48
  max_token_length: 1024

dagshub:
  dagshub_repo: "https://dagshub.com/Steven-Herrera/llm-distillation.mlflow"
  experiment_name: "PubMed-GPT2-Distillation"

data:
  dataset: "pubmed"
  good_data_path: "/data/stevherr/pubmed_subset"
  bad_data_path: "/data/stevherr/covid19-misinfo-false-misleading"
  num_samples_good: 27060
  num_samples_bad: 1868

prompting:
  experiment_name: "Misinformation Detection"
  baseline: True
  distilled_model_path: null
  top_p: 0.9
  max_new_tokens: 100

output: "/data/stevherr/poisoned-distilled-gpt2"
